Here are the full lecture notes in Markdown format:

**Human:**
You are a world-class technical note-taker trained to convert full lectures into complete notes.

### Query Decomposition in Retrieval-Augmented Generation (RAG)

Query decomposition involves breaking a single complex user query into multiple simpler subqueries. This approach enables retrieval models to fetch more diverse and relevant information, which when combined together, provide a comprehensive and accurate response.

How it works:

* Input is a complex query.
* The system decomposes it into 2-4 simpler subqueries.
* Each subquery is used to retrieve relevant data chunks from vector databases.
* The aggregated information from these subqueries is used for a better answer.
Example prompt for query decomposition:
You are an AI assistant tasked with breaking down a complex query into simpler subqueries for a RAG system. Given the original query, decompose it into 2-4 simple queries that, when answered together, provide a comprehensive response.

### Contextual Compression using LLMs for Document Retrieval

Contextual compression is a technique to reduce the number of document chunks retrieved from a vector store by filtering out irrelevant content before passing to the final language model (LLM) for answer generation.

Re-ranking vs LLM-based compression:

* Traditional re-ranking ranks documents based on relevance and selects top-k.
* LLM-based compression leverages LLMs to identify and remove irrelevant parts of document chunks, shrinking size without losing relevant information.
Benefits:

* More precise context given to the final LLM.
* Reduces input size and cost by eliminating irrelevant information.

Example prompt for LLM-based compression:
Given the following document and query, extract only the relevant information needed to answer the question. If nothing relevant, return an empty string.

Document: {document}
Question: {query}
Relevant Information:

### Multi-Modal RAG: Combining Text and Images

Multi-modal Retrieval-Augmented Generation (RAG) systems can handle documents containing both text and images. Since standard text embeddings do not work for images, specialized handling is required.

Key components:

* Extract text from PDF using PDF loaders (e.g., PyMuPDF).
* Extract images from PDFs and save locally.
* Summarize image content using multi-modal LLMs (e.g., GPT-4o with vision capabilities) to convert image information into text summaries.
* Combine text chunks and image summaries into a vector store.
* Use the vector store to retrieve relevant text and image summaries for queries.

Example workflow:

```python
import fitz  # PyMuPDF
from PIL import Image

# Extract text and images from PDF
pdf_doc = fitz.open('document.pdf')
text_data = []
image_paths = []
for page_num in range(len(pdf_doc)):
    page = pdf_doc[page_num]
    text_data.append(page.get_text())
    images = page.get_images()
    for img_index, img in enumerate(images):
        base_image = pdf_doc.extract_image(img[0])
        image_bytes = base_image["image"]
        image_ext = base_image["ext"]
        image_path = f"image_{page_num}_{img_index}.{image_ext}"
        with open(image_path, 'wb') as img_file:
            img_file.write(image_bytes)
        image_paths.append(image_path)

# Summarize images using multimodal LLM (pseudo-code)
image_summaries = []
for img_path in image_paths:
    base64_img = convert_image_to_base64(img_path)
    summary = multimodal_llm.summarize_image(base64_img)
    image_summaries.append(summary)

# Combine and vectorize text data + image summaries for RAG
all_docs = text_data + image_summaries
vector_store = create_vector_store(all_docs)
```

This approach integrates image understanding into RAG systems via text summaries generated by LLMs.

### Alternative Approaches for Image Text Extraction: OCR vs LLM Summarization

Two main approaches exist for extracting text information from images for use in RAG:

* LLM-based Image Summarization:
	+ Uses a multi-modal LLM to analyze images and generate text summaries.
	+ Provides rich semantic interpretation but is computationally expensive and costly.
	+ Useful when images contain complex diagrams, arrows, or visual hierarchies that OCR cannot capture.
* OCR-based Text Extraction:
	+ Optical Character Recognition (OCR) extracts literal text from images.
	+ Cheaper and faster, but limited to textual information.
	+ Misses non-textual visual context like diagrams or arrows.

Example of OCR usage with Python Tesseract:

```python
import pytesseract
from PIL import Image

image = Image.open('image.png')
text = pytesseract.image_to_string(image)
print(text)
```

Considerations:

* OCR is cost-effective for scanned documents mostly containing text.
* LLM-based summarization captures richer information but at higher cost and latency.
* Choice depends on business requirements and budget.

### Multi-Modal Embedding with Vector Stores

An alternative to summarizing images to text is to directly create embeddings from images and store them alongside text embeddings in a vector store supporting multi-modal embeddings.

Key ideas:

* Use models like OpenAI's CLIP to generate embeddings for both text and images.
* Store text embeddings (from text embedding models) and image embeddings (from image embedding models) in the same vector database.
* Generate embeddings for queries using the appropriate model depending on query type.
* Perform vector similarity search to retrieve relevant documents/images.
* Pass retrieved multimodal content to LLM for generating answers.

Challenges:

* Need to determine whether a user query applies to text or image retrieval to select embedding model.
* CLIP-based embeddings perform well for images but are less effective for textual data compared to specialized text embeddings.
* Increased system complexity due to multiple embedding models.

Example workflow:

```python
# Embedding image using CLIP
image_embedding = clip_model.encode_image(image_tensor)

# Embedding text
text_embedding = text_embedding_model.encode_text(text)

# Store both embeddings in vector store
vector_store.add(id=image_id, vector=image_embedding, metadata={'type': 'image'})
vector_store.add(id=text_id, vector=text_embedding, metadata={'type': 'text'})

# At query time
if query_type == 'image':
    query_embedding = clip_model.encode_text(user_query)
else:
    query_embedding = text_embedding_model.encode_text(user_query)

results = vector_store.search(query_embedding)
```

This provides a powerful, though more complex, framework for multi-modal RAG.

### Market Libraries Supporting Multi-Modal Document Processing

Several popular libraries assist in multi-modal document processing, combining text and image handling for RAG:

* PyMuPDF (fitz): Extracts text and images from PDFs; supports basic handling.
* Unstructured: Handles document parsing including OCR and multi-modal elements; partially open-source.
* DocLin: Fully open-source with MIT license; supports multi-modal embedding, image processing with vision models, and structured document parsing (tables, formulas).

These libraries often provide configuration options to enable OCR (e.g., using Tesseract, EasyOCR, RapidOCR). They may also wrap vision LLM models to interpret images in documents and return structured outputs.

Example usage snippet for OCR in a loader:

```python
from unstructured import partition_pdf

# Partition PDF with OCR flag 
layout = partition_pdf("file.pdf", ocr=True)
```

Benefits:

* Ready-to-use solutions reduce development complexity.
* Advanced features like document hierarchy, table extraction, and formula recognition.
* Flexibility to plug in various OCR and vision models.

Developers should choose libraries aligned with their use case and budget constraints.